import numpy as np
import onnxruntime as rt
import tf2onnx
import tensorflow as tf

# --- 1. ì €ì¥ëœ 256x256 .keras ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ ---
# íŒŒì¼ ì´ë¦„ì„ ìƒˆë¡œ ìƒì„±ëœ ëª¨ë¸ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë³€ê²½
model_path = "unet_mobilenetv2_256.keras"
model = tf.keras.models.load_model(model_path)
print("âœ… TensorFlow ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")


# --- 2. ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ ê°ì²´ë¥¼ ì‚¬ìš©í•´ ONNXë¡œ ë³€í™˜í•©ë‹ˆë‹¤ ---

# ëª¨ë¸ì˜ ì…ë ¥ ì‚¬ì–‘ì„ 256x256ìœ¼ë¡œ ì§ì ‘ ì •ì˜í•©ë‹ˆë‹¤.
input_signature = [tf.TensorSpec([1, 256, 256, 3], tf.float32, name="input")]

tf2onnx.convert.from_keras(
    model=model,
    input_signature=input_signature,  # ìˆ˜ì •ëœ ì…ë ¥ ì‚¬ì–‘ ì§€ì •
    output_path="unet_mobilenetv2_256.onnx", # ì¶œë ¥ íŒŒì¼ ì´ë¦„ë„ ë³€ê²½
    opset=16
)
print("âœ… ëª¨ë¸ì„ 'unet_mobilenetv2_256.onnx' íŒŒì¼ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")


# --- 3. ë³€í™˜ëœ ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---
print("\nğŸ”¬ ë³€í™˜ëœ ONNX ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")

# ONNX Runtime ì„¸ì…˜ ì‹œì‘
sess = rt.InferenceSession("unet_mobilenetv2_256.onnx")

# ì…ë ¥ ì´ë¦„ê³¼ ì¶œë ¥ ì´ë¦„ í™•ì¸
input_name = sess.get_inputs()[0].name
output_name = sess.get_outputs()[0].name
print(f"ì…ë ¥ ë ˆì´ì–´ ì´ë¦„: {input_name}")
print(f"ì¶œë ¥ ë ˆì´ì–´ ì´ë¦„: {output_name}")

# ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ëŠ” 256x256 ë”ë¯¸ ë°ì´í„° ìƒì„±
dummy_input = np.random.randn(1, 256, 256, 3).astype(np.float32)

# ONNX ëª¨ë¸ë¡œ ì¶”ë¡  ì‹¤í–‰
result = sess.run([output_name], {input_name: dummy_input})[0]

# ê²°ê³¼ í™•ì¸
print(f"\nì¶”ë¡  ê²°ê³¼ì˜ í˜•íƒœ(Shape): {result.shape}")
# ìµœì¢… ì¶œë ¥ í˜•íƒœê°€ (1, 256, 256, 3)ì´ ë©ë‹ˆë‹¤.
print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ í˜•íƒœê°€ (1, 256, 256, 3)ìœ¼ë¡œ ë‚˜ì˜¤ë©´ ì„±ê³µì…ë‹ˆë‹¤.")
